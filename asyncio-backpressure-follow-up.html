<!DOCTYPE html>
<html lang="en">
        <head>
                        <meta charset="utf-8" />
                        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
                        <meta name="generator" content="Pelican" />
                        <title>Asyncio backpressure - follow up</title>
                        <link rel="stylesheet" href="./theme/css/main.css" />
                                <link href="https://blog.changs.co.uk/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Jamie's Blog Atom Feed" />
                        <!-- Cloudflare Web Analytics --><script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "0358c4be608641c1ad90cd87801da29b"}'></script><!-- End Cloudflare Web Analytics -->
    <meta name="description" content="Previously when discussing asyncio backpressure I've made some claims that were not necessarily complete. I said: It works well for 100s of urls..." />
        </head>

        <body id="index" class="home">
                <header id="banner" class="body">
                        <h1><a href="./">Jamie's Blog</a></h1>
                        <nav><ul>
                                                <li class="active"><a href="./category/blog.html">Blog</a></li>
                        </ul></nav>
                </header><!-- /#banner -->
  <section id="content" class="body">
    <article>
      <header>
        <h1 class="entry-title">
          <a href="./asyncio-backpressure-follow-up.html" rel="bookmark"
             title="Permalink to Asyncio backpressure - follow up">Asyncio backpressure - follow up</a></h1>
      </header>

      <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2025-09-14T00:00:00+01:00">
                Published: Sun 14 September 2025
        </abbr>

                <address class="vcard author">
                        By                                 <a class="url fn" href="./author/jamie-chang.html">Jamie Chang</a>
                </address>
        <p>In <a href="./category/blog.html">Blog</a>.</p>
        
</footer><!-- /.post-info -->        <p>Previously when discussing <a href="./asyncio-backpressure-processing-lots-of-tasks-in-parallel.html">asyncio backpressure</a> I've made some claims that were not necessarily complete.</p>
<p>I said:</p>
<blockquote>
<p>It works well for 100s of urls but when we hit a big number like 10000s we have a problem.</p>
<p>The program seemingly hangs. This is because all the tasks are being created first and only then to do allow the tasks to start executing. The program will also use much more memory than it needs and generally might slow down due to more context switching. Definitely not what we want</p>
</blockquote>
<p>There are 2 issues here:</p>
<ul>
<li>First about the program hanging, it's actually quite hard to observe that behaviour. </li>
<li>Second about the numbers, 10000s of tasks is actually not a big number for asyncio.</li>
</ul>
<p>This was pointed out in a <a href="https://github.com/Jamie-Chang/aiointerpreters/issues/3">Github issue</a> by <a href="https://github.com/bmwant">Misha Behersky</a>. Additionally I had not made it very clear why I proposed using semaphore like:</p>
<div class="highlight"><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">process_all</span><span class="p">(</span><span class="n">urls</span><span class="p">:</span> <span class="n">Iterable</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">semaphore</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">Semaphore</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
    <span class="k">async</span> <span class="k">with</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">TaskGroup</span><span class="p">()</span> <span class="k">as</span> <span class="n">tg</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="k">await</span> <span class="n">semaphore</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
            <span class="n">tg</span><span class="o">.</span><span class="n">create_task</span><span class="p">(</span><span class="n">process_one</span><span class="p">(</span><span class="n">url</span><span class="p">))</span><span class="o">.</span><span class="n">add_done_callback</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">semaphore</span><span class="o">.</span><span class="n">release</span><span class="p">())</span>
</code></pre></div>

<p>As opposed to using it in a more standard way:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">process_one</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">semaphore</span><span class="p">):</span>
    <span class="k">async</span> <span class="k">with</span> <span class="n">semaphore</span><span class="p">:</span>
        <span class="o">...</span>
</code></pre></div>

<p>In order to properly explain the behaviour, I needed to construct a much better example so that I can benchmark both memory and speed.</p>
<h2>Simulation</h2>
<p>First and foremost, I've been testing back pressure by making concurrent calls to fetch wikipedia articles. </p>
<div class="highlight"><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">request</span><span class="p">(</span><span class="n">client</span><span class="p">:</span> <span class="n">httpx</span><span class="o">.</span><span class="n">AsyncClient</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">await</span> <span class="n">client</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="o">...</span>
</code></pre></div>

<p>This is a good real world test, but when we are experimenting with thousands of concurrent calls, it adds load to unsuspecting servers. </p>
<p>I could set up my own servers, but we have a simpler choice.</p>
<p>One of the benefits of asyncio is that it provides primitives for concurrent operations. We can in that case easily simulate the network latency using <code>asyncio.sleep</code>:</p>
<div class="highlight"><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">request</span><span class="p">(</span><span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">))</span>  <span class="c1"># simulate the time delay for getting requests</span>
</code></pre></div>

<p>Here we assume the latency is between 100 and 200 ms. But we can also extend this to match a distribution observe in the real world. Similarly we can use this to simulate load with databases and message queues.</p>
<h2>Measuring the speed and memory</h2>
<p>Testing the speed or duration is simple, the best way is using <a href="https://docs.python.org/3/library/time.html#time.perf_counter">time.perf_counter</a>. We can compose this into a context manager like this:</p>
<div class="highlight"><pre><span></span><code><span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">timer</span><span class="p">(</span><span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="kc">None</span><span class="p">]:</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">,</span> <span class="s2">&quot;s&quot;</span><span class="p">)</span>
</code></pre></div>

<p>Similarly we can track the peak memory used by using <a href="https://docs.python.org/3/library/tracemalloc.html">tracemalloc</a>:</p>
<div class="highlight"><pre><span></span><code><span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">memory_profiler</span><span class="p">(</span><span class="n">message</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="kc">None</span><span class="p">]:</span>
    <span class="n">tracemalloc</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">peak_memory</span> <span class="o">=</span> <span class="n">tracemalloc</span><span class="o">.</span><span class="n">get_traced_memory</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Peak memory usage: </span><span class="si">{</span><span class="n">peak_memory</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> bytes&quot;</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">tracemalloc</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</code></pre></div>

<h2>Testing different implementations</h2>
<p>In the original article, I proposed both batching and semaphore as methods of back pressure. So our test scenarios are as follows </p>
<ol>
<li>batched execution.</li>
<li>acquiring semaphore inside the task as Python intended.</li>
<li>acquiring semaphore before task creation and releasing with callback as I proposed.</li>
</ol>
<p>Finally a bonus implementation suggested in the issue thread to release the semaphore at the end of the function call:</p>
<div class="highlight"><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">semaphore</span> <span class="o">=</span> <span class="n">Semaphore</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">with</span> <span class="n">TaskGroup</span><span class="p">()</span> <span class="k">as</span> <span class="n">tg</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="k">await</span> <span class="n">semaphore</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
            <span class="n">tg</span><span class="o">.</span><span class="n">create_task</span><span class="p">(</span><span class="n">process</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">semaphore</span><span class="p">))</span>


<span class="k">async</span> <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">semaphore</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="o">...</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">semaphore</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
</code></pre></div>

<p>The code can be found <a href="https://github.com/Jamie-Chang/concurreny-bench">here</a>. We run 100,000 inputs for each implementation limiting concurrency to 500 at a time.</p>
<h2>Results</h2>
<table>
<thead>
<tr>
<th>File</th>
<th>Description</th>
<th>Memory usage in bytes</th>
<th>Duration in seconds</th>
</tr>
</thead>
<tbody>
<tr>
<td>batching.py</td>
<td>Using batched to process things in batches as opposed to using semaphores</td>
<td>821,692</td>
<td>41.7</td>
</tr>
<tr>
<td>semaphore1.py</td>
<td>Traditional pythonic way of using semaphore, does not limit task creation</td>
<td>152,154,848</td>
<td>30.9</td>
</tr>
<tr>
<td>semaphore2.py</td>
<td>Semaphore limiting task creation with callback</td>
<td>1,018,796</td>
<td>30.2</td>
</tr>
<tr>
<td>semaphore3.py</td>
<td>Semaphore limiting task creation with release called in the task function</td>
<td>839,176</td>
<td>30.2</td>
</tr>
</tbody>
</table>
<h2>Analysis</h2>
<p>Traditional use of semaphore does not limit the number of tasks being created, we must first create all 100,000 tasks and then start processing the tasks. As a result, the amount of memory requires is many times higher. Though the difference in speed is fairly small, it is consistently observable as we are not able to start processing during the creation phase.</p>
<p>Another interesting result is that the semaphore2 with the callback uses more memory than just passing the semaphore to release in semaphore3. This is a quirk of constructing an extra lambda as a callback. Additionally, it may be more obvious than using the callback, so this method is definitely worth considering.</p>
<p>Finally I want to talk about batching. Batching is very memory efficient as it doesn't need any extra objects however it does take a lot longer.
This is explained in my original post:</p>
<blockquote>
<p>The problem is if we have a small amount of tasks with long wait times then it'll slow down the whole batch.</p>
</blockquote>
<p>Notice that our distributions has a lot of variance:</p>
<div class="highlight"><pre><span></span><code><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
</code></pre></div>

<p>if we reduce this to <code>random.uniform(0.1, 0.11)</code> we have closer results</p>
<table>
<thead>
<tr>
<th>File</th>
<th>Duration in seconds</th>
</tr>
</thead>
<tbody>
<tr>
<td>batching.py</td>
<td>24.65</td>
</tr>
<tr>
<td>semaphore1.py</td>
<td>22.0</td>
</tr>
<tr>
<td>semaphore2.py</td>
<td>21.1</td>
</tr>
<tr>
<td>semaphore3.py</td>
<td>21.1</td>
</tr>
</tbody>
</table>
<p>My view is batching and the 2 semaphore solutions are all very good solutions. Batching is simple, but you can get more performance out of using semaphores just not in the obvious way.</p>
<p>On the other hand using semaphore normally is good but you should be aware of the memory implication.</p>
<h2>Finally</h2>
<p>I've spent a lot of time investigating asyncio back pressure here. The point is to provide the full context of the different options to use. </p>
<p>I also hope that I've provided some ideas around how you might investigate different design patterns yourself.</p>
      </div><!-- /.entry-content -->

    </article>
  </section>
                <section id="extras" class="body">
                                <div class="social">
                                        <h2>social</h2>
                                        <ul>
                                                        <li><a href="https://blog.changs.co.uk/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                                                        <li><a href="https://github.com/Jamie-Chang/">Github</a></li>
                                                        <li><a href="https://www.linkedin.com/in/jamie-chang-4423ba125/">LinkedIn</a></li>
                                                        <li><a href="https://bsky.app/profile/changs.co.uk/">Bluesky</a></li>
                                        </ul>
                                </div><!-- /.social -->
                </section><!-- /#extras -->

                <footer id="contentinfo" class="body">
                        <address id="about" class="vcard body">
                                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                        </address><!-- /#about -->

                        <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
                </footer><!-- /#contentinfo -->

        </body>
</html>